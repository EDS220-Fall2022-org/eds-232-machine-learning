---
title: "Cutler_Lab2"
author: "Victoria Cutler"
date: "2023-01-18"
output: 
  pdf_document:
      pandoc_args: --listings
      includes:
        in_header: preamble.tex
  html_document: default
editor_options: 
  markdown: 
    wrap: 72
---

Today we will be continuing the pumpkin case study from last week. We
will be using the data that you cleaned and split last time
(pumpkins_train) and will be comparing our results today to those you
have already obtained, so open and run your Lab 1 .Rmd as a first step
so those objects are available in your Environment (unless you created
an R Project last time, in which case, kudos to you!).

Once you have done that, we'll start today's lab by specifying a recipe
for a polynomial model. First we specify a recipe that identifies our
variables and data, converts package to a numerical form, and then add a
polynomial effect with step_poly()

```{r, results = 'hide', message = FALSE, warning = FALSE, fig.show = 'hide'}
# Sourcing code from Lab 1
sourceDir <- "/Users/victoriacutler/Documents/MEDS/Courses/EDS232/Lab1.Rmd"
library(knitr)
source(knitr::purl(sourceDir, quiet=TRUE))             
```

```{r}
# Specify a recipe
poly_pumpkins_recipe <-
  recipe(price ~ package, data = pumpkins_train) %>%
  step_integer(all_predictors(), zero_based = TRUE) %>% 
  step_poly(all_predictors(), degree = 4)
```

How did that work? Choose another value for degree if you need to. Later
we will learn about model tuning that will let us do things like find
the optimal value for degree. For now, we'd like to have a flexible
model, so find the highest value for degree that is consistent with our
data.

Polynomial regression is still linear regression, so our model
specification looks similar to before.

```{r}
# Create a model specification called poly_spec
poly_spec <- linear_reg() %>% 
  set_engine("lm") %>% 
  set_mode("regression")
```

Question 1: Now take the recipe and model specification that just
created and bundle them into a workflow called poly_wf.

```{r, echo = FALSE}
print("Answer 1. See below workflow code.")
```


```{r}
# Bundle recipe and model spec into a workflow
poly_wf <- workflow() |> 
  add_recipe(poly_pumpkins_recipe) |> 
  add_model(poly_spec)
```

Question 2: fit a model to the pumpkins_train data using your workflow
and assign it to poly_wf_fit

```{r, echo = FALSE}
print("Answer 2. Below, you'll see a. code for how we fit the model to the pumpkins training dataset, b. the corresponding model coefficients after fitting this polynomial regression model to the data, then c. 10 instances of model predictions of price (denoted as '.pred') on the test set compared to actual values of price (denoted as 'price').")
```


```{r}
# a. fit a model
poly_wf_fit <- poly_wf |> 
  fit(data = pumpkins_train)
```

```{r}
# b. print learned model coefficients
poly_wf_fit
```

```{r}
# c. 
  # Make price predictions on test data
  poly_results <- poly_wf_fit %>% predict(new_data = pumpkins_test) %>% 
    bind_cols(pumpkins_test %>% select(c(package, price))) %>% 
    relocate(.pred, .after = last_col())
  
  # Print the results
  poly_results %>% 
    slice_head(n = 10)
```

Now let's evaluate how the model performed on the test_set using
yardstick::metrics().

```{r}
metrics(data = poly_results, truth = price, estimate = .pred)
```

Question 3: How do the performance metrics differ between the linear
model from last week and the polynomial model we fit today? Which model
performs better on predicting the price of different packages of
pumpkins?

Let's visualize our model results. First prep the results by binding the
encoded package variable to them.
```{r, echo=FALSE}
print("Answer 3. These performance metrics illustrate that the polynomial model fits and predicts the price of  different pumpkin packages better than the linear model. We know this since the RMSE and MAE are approximately halved in this model compared to the linear model, and the R-squared is approximately doubled. In other words, with the polynomial model, our error between what the model is predicting and what the true values are, is much smaller, and the variability in our data is much better explained by the relationship of features within this polynomial model.")
```

```{r}
# Bind encoded package column to the results
poly_results <- poly_results |>  
  bind_cols(package_encode |> 
              rename(package_integer = package)) |>  
  relocate(package_integer, .after = package)

# Print new results data frame
poly_results %>% 
  slice_head(n = 5)
```

OK, now let's take a look!

Question 4: Create a scatter plot that takes the poly_results and plots
package vs. price. Then draw a line showing our model's predicted values
(.pred). Hint: you'll need separate geoms for the data points and the
prediction line.
```{r, echo = FALSE}
print("Answer 4. See the below scatter plot to see the relationship between pumpkin package and price. The orange line represents the model's predicted values and we can see that although the polynomial regression is doing a better job predicting pumpkin price given pumpkin package, this line is not predicting the majority of the test data very accurately. Perhaps an algorithm better suited for categories, such as a classification model, would be better.")
```

```{r}
# Make a scatter plot
poly_results %>% ggplot(mapping = aes(x = package_integer, y = price)) +
  geom_point() +
  geom_line(mapping = aes(y = .pred), color = "orange", size = 1.2)
```

You can see that a curved line fits your data much better.

Question 5: Now make a smoother line by using geom_smooth instead of
geom_line and passing it a polynomial formula like this:
geom_smooth(method = lm, formula = y \~ poly(x, degree = 3), color =
"midnightblue", size = 1.2, se = FALSE)

```{r, echo = FALSE}
print("Answer 5. See the below scatter plot to see the relationship between pumpkin package and price. This time, the blue line represents the a 3rd degree polynomial regression that models the data. The smooth line looks to fit and predict the data a bit better, but we still see that a classification model may be better suited for our data.")
```


```{r}
# Make a smoother scatter plot 
poly_results %>% ggplot(mapping = aes(x = package_integer, y = price)) +
  geom_point() +
  geom_smooth(method = lm, formula = y ~ poly(x, degree = 3), color = "midnightblue", size = 1.2, se = FALSE)
```

OK, now it's your turn to go through the process one more time.

Additional assignment components :

6.  Choose a new predictor variable (anything not involving package
    type) in this dataset.

```{r, echo = FALSE}
print("Answer 6. I am selecting variety as my new predictor variable since this variable is highly correlated with price.")
```

7.  Determine its correlation with the outcome variable (price).
    (Remember we calculated a correlation matrix last week)

```{r}
cor_variety <- cor(baked_pumpkins$variety, baked_pumpkins$price)
```

```{r, echo = FALSE}
print(paste0("Answer 7. The correlation between pumpkin variety and pumpkin price is approx: ", round(abs(cor_variety), 2)))
```

8.  Create and test a model for your new predictor: - Create a recipe -
    Build a model specification (linear or polynomial) - Bundle the
    recipe and model specification into a workflow - Create a model by
    fitting the workflow - Evaluate model performance on the test data -
    Create a visualization of model performance
    
```{r, echo = FALSE}
print("Answer 8. See the below code and visual for how the model performs on predicting the price given variety.")
```

```{r}
# create recipe
poly_pumpkins_recipe_variety <-
  recipe(price ~ variety, data = pumpkins_train) %>%
  step_integer(all_predictors(), zero_based = TRUE) %>% 
  step_poly(all_predictors(), degree = 3)

# Create model specification
poly_spec_variety <- linear_reg() %>% 
  set_engine("lm") %>% 
  set_mode("regression")

# Bundle recipe and model spec into a workflow
poly_wf_variety <- workflow() |> 
  add_recipe(poly_pumpkins_recipe_variety) |> 
  add_model(poly_spec_variety)

# fit a model
poly_wf_fit_variety <- poly_wf_variety |> 
  fit(data = pumpkins_train)

# Make price predictions on test data
poly_results_variety <- poly_wf_fit_variety %>% predict(new_data = pumpkins_test) %>% 
  bind_cols(pumpkins_test %>% select(c(variety, price))) %>% 
  relocate(.pred, .after = last_col())
```

```{r}
# build a linear recipe to extract encoded variables
lm_pumpkins_recipe_variety <- recipe(price ~ variety, data = pumpkins_train) %>% 
  step_integer(all_predictors(), zero_based = TRUE)

# create encode variety column
variety_encode <- lm_pumpkins_recipe_variety %>% 
  prep() %>% 
  bake(new_data = pumpkins_test) %>% 
  select(variety)

# Bind encoded variety column to the results
poly_results_variety <- poly_results_variety |>  
  bind_cols(variety_encode |> 
              rename(variety_integer = variety)) |>  
  relocate(variety_integer, .after = variety)

# Print new results data frame
poly_results_variety %>% 
  slice_head(n = 5)
```
```{r, echo = FALSE}
print("The below metrics show similar model performance as using package to predict price. Perhaps using a combination of these features could lead to significant model performance improvement.")
```

```{r}
metrics(data = poly_results_variety, truth = price, estimate = .pred)
```

```{r}
poly_results_variety %>% ggplot(mapping = aes(x = variety_integer, y = price)) +
  geom_point() +
  geom_line(mapping = aes(y = .pred), color = "orange", size = 1.2)
```

Lab 2 due 1/24 at 11:59 PM
