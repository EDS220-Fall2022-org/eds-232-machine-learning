---
title: "Lab5_Demo"
author: "Mateo Robbins"
date: "2023-02-05"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)    
library(ggplot2)   
library(rsample)   
library(recipes)
library(skimr)
library(tidymodels)
library(kknn)
```

###k-nearest neighbor in tidymodels

## Data

```{r data}
data(attrition)
churn <- attrition %>% mutate_if(is.ordered, .funs = factor, ordered = F) 
#skim(churn_dat)
```

Not doing the data exploration here in the interest of time and since we are familiar with this dataset.

```{r initial_split}
set.seed(123)
#initial split of data, default 75/25
```

We need to create a recipe and do the preprocessing by converting dummy coding the nominal variables and normalizing the numeric variables.

```{r recipe}
#preprocessing
knn_rec 

#bake 
  baked_churn 
```

Recall: if you want to explore the what the recipe is doing to your data, you can first prep() the recipe to estimate the parameters needed for each step and then bake(new_data = NULL) to pull out the training data with those steps applied.

Now the recipe is ready to be applied to the test data.

```{r bake_test}

```

##Specify the k-nearest neighbor model

```{r knn_spec}
knn_spec
```

```{r}
knn_fit <- knn_spec %>% 
  fit(Attrition ~. , data = churn_train)
```


```{r cv}
set.seed(123)
# 10-fold CV on the training dataset
cv_folds 

```

We now have a recipe for processing the data, a model specification, and CV splits for the training data.

Let's put it all together in a workflow.

```{r}
knn_workflow 
```

Now fit the resamples.
```{r}
knn_res 
```

# Check the performance

```

```{r spec_with_tuning}
# Define our KNN model with tuning
knn_spec_tune 

# Check the model
knn_spec_tune
```


```{r}
# Define a new workflow
wf_knn_tune
    
# Fit the workflow on our predefined folds and hyperparameters
fit_knn_cv 
    
# Check the performance with collect_metrics()
fit_knn_cv %>% collect_metrics()
```

This time before we fit the model we need to tell R which values to try for the parameter that we're tuning.

To tune our hyperparameter(s), we will use the tune_grid() function (instead of the fit() or fit_resamples() functions).

This tune_grid() is similar to fit_resamples() except that it takes an additional argument: grid. We will pass the possible values of our hyperparameter(s) to this grid argument, and it will evaluate each fold of our sample on each set of hyperparameters passed to grid.

```{r evaluate_model}
knn_fit 
```

```{r}
# Define the workflow and fit the model on our predefined folds
fit_churn_cv 
# Check the performance
fit_churn_cv %>% collect_metrics()
```

And finally, we will predict.

The finalize_workflow() function wants (1) your initial workflow and (2) your best model.

```{r}
# The final workflow for our KNN model
final_wf 

# Check out the final workflow object
final_wf
```

```{r}
# Fitting our final workflow
final_fit 
# Examine the final workflow
final_fit
```

And finally, we can predict onto the testing dataset.

```{r}
churn_pred

churn_pred %>% head()
```

There's a better way! You can pass your final workflow (workflow plus the best model) to the last_fit() function along with your initial split (for us: churn_split) to both (a) fit your final model on your full training dataset and (b) make predictions onto the testing dataset (defined in your initial split object).

This last_fit() approach streamlines your work (combining steps) and also lets you easily collect metrics using the collect_metrics() function

```{r}
# Write over 'final_fit' with this last_fit() approach
final_fit 
# Collect metrics on the test data!
final_fit 
```

